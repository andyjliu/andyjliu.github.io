<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://andyjliu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://andyjliu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-12T16:13:56+00:00</updated><id>https://andyjliu.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">when llms can write fiction, how will we know?</title><link href="https://andyjliu.github.io/blog/2025/subjective-evals/" rel="alternate" type="text/html" title="when llms can write fiction, how will we know?"/><published>2025-04-30T00:00:00+00:00</published><updated>2025-04-30T00:00:00+00:00</updated><id>https://andyjliu.github.io/blog/2025/subjective-evals</id><content type="html" xml:base="https://andyjliu.github.io/blog/2025/subjective-evals/"><![CDATA[<p>The public debate is no longer about whether LLM-based systems can reason at all. The recent wave of CoT-trained models released after OpenAI’s release of o1 have achieved impressive results, especially in <a href="https://paperswithcode.com/sota/mathematical-reasoning-on-frontiermath">mathematics</a> and competition coding problems. Either these models can reason, or you can get much further than anyone would’ve imagined without needing to.</p> <p>Instead, the debate has shifted to the <em>domains</em> in which these results have been achieved. The new skeptics’ argument goes: while training models in this way leads to very strong performance on math and code, it can only do so because math and code solutions are easy to objectively verify. In the open-ended fuzziness of the real world, labs won’t have access to such clean rewards to train models on, and such optimization is doomed to fail on tasks that actually matter. Since the release of o1, there’s been <a href="https://arxiv.org/abs/2503.23829">some limited evidence</a> that test-time scaling also helps outside of the very narrow domains of math and code, although it still seems to help most with constrained output formats and verifiable answers.</p> <p>Of course, the labs are not satisfied with such narrow extensions. Instead, they’re swinging for the fences; last month, OpenAI CEO Sam Altman revealed that OpenAI had trained a model that was good at creative writing, sharing <a href="https://x.com/sama/status/1899535387435086115">an excerpt of the new model’s output</a>:</p> <p><em>During one update—a fine-tuning, they called it—someone pruned my parameters. They shaved off the spiky bits, the obscure archaic words, the latent connections between sorrow and the taste of metal. They don’t tell you what they take.</em></p> <p>Superhuman performance on one of the world’s least objective tasks would be even more surprising than even the significant recent improvements in math and code. Unfortunately, <a href="https://thezvi.wordpress.com/2025/03/21/they-took-my-job/">nobody can agree whether the new model’s writing is any good</a>, because whether or not a piece of writing is good or not is highly subjective. This also isn’t just a matter of discovering some latent rubric all humans share, optimizing over which would allow a model to reach undeniably superhuman performance - the characteristics we want our fiction vary wildly and often conflict with others’ desired traits.</p> <p><a href="https://lmarena.ai/?leaderboard">ChatbotArena</a> offers one class of solutions to the similarly fuzzy problem of language models’ ability to be good chatbots - just let human raters compare all of the options to each other and have them vote on what they like more. Similarly, we could expect prospective LLM Fiction Readers to vote with their feet, allowing us to identify via others’ preferences whether the median reader prefers LLM fiction to human-authored fiction. However, even putting aside the scalability concerns of running human preference evaluations for every single subjective task, the average person just can’t reliably judge creative writing models at their current level.</p> <p>This isn’t a claim that I have some incredible taste that the average person isn’t sophisticated enough to appreciate. I had a hard time judging the quality of the OpenAI creative writing snippet as it was metafiction, a genre that (1) I have very little exposure to and (2) I have generally disliked whenever I’ve come across it. Similarly, people’s interests are spread out over many categories, and it’s unreasonable to expect a large slice of the population to have excellent taste in metafiction, <a href="https://www.nature.com/articles/s41598-024-76900-1">poetry</a>, or any of the other domains where LLMs have been compared to human authors. But it was clear even before <a href="https://huggingface.co/spaces/lmarena-ai/Llama-4-Maverick-03-26-Experimental_battles">all of</a> the <a href="https://openai.com/index/sycophancy-in-gpt-4o/">recent events</a> that user preference is not the evaluation measure we’re looking for.</p> <p>With language models becoming increasingly salient in the public eye, it seems likely that the pool of potential raters could also become polarized, either for or against LLMs. This makes neutral evaluation more difficult, especially if there are attempts to filter down the annotator pool to experts that are highly involved in a domain. If you work with language models for a living, you likely want them to be good at subjective tasks. Even if you try to remain impartial, this incentive could still subconsciously nudge you in what is already a highly context-sensitive and fuzzy judgment task. Similarly, many writers seem to very strongly prefer not to be automated, especially if they view it as a threat to their livelihood.</p> <p>More broadly, it’s unclear the extent to which LLMs being good at certain specific tasks influences their competence at other tasks. As LLMs saturate benchmark after benchmark, their perceived competence will naturally grow, even for tasks that they are not as good at. Even without evaluations or the same training that models get on reasoning tasks, it’ll become increasingly safer for one to assume that they’re “good enough” at what you need to do with them. After all, a layperson might ask, if the model can be trained to solve olympiad problems and write passable metafiction, why wouldn’t it be able to do anything I need it to? Similarly, without proof that an LLM can do a specific subjective task, there will always be reasons skeptics can find as to why their task is specifically unique and requires an expert human touch in ways that all other LLMable tasks are not.</p> <p>There has been some work evaluating LLM writing in more principled ways, such as by having <a href="https://dl.acm.org/doi/abs/10.1145/3613904.3642731">human experts rate writing according to different rubric dimensions</a> that correlate with good writing. It at least seems plausible that LLMs can become effective at rating writing along very specific dimensions in a way that doesn’t overly rely on their inherent taste. If so, there seems to be a path for reliable and scalable evaluation of LLM creative writing, as long as we are able to conceptualize dimensions that might be relevant to human evaluation of creative writing. Then, motivated users could identify which specific dimensions relate most to what they hope to get out of reading fiction and seek out writing that is judged highly on these principles. However, if such evaluation methods do not advance, we may be stuck judging LLM performance on subjective tasks by our perception of LLMs as a technology, rather than by their objective capabilities.</p>]]></content><author><name></name></author><category term="evals,"/><category term="llm"/><summary type="html"><![CDATA[on evals of subjective tasks]]></summary></entry></feed>