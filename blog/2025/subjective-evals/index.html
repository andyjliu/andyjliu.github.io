<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> when llms can write fiction, how will we know? | Andy Liu </title> <meta name="author" content="Andy Liu"> <meta name="description" content="on evals of subjective tasks"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://andyjliu.github.io/blog/2025/subjective-evals/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Andy</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">when llms can write fiction, how will we know?</h1> <p class="post-meta"> Created on April 30, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/evals"> <i class="fa-solid fa-hashtag fa-sm"></i> evals,</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>The public debate is no longer about whether LLM-based systems can reason at all. The recent wave of CoT-trained models released after OpenAI’s release of o1 have achieved impressive results, especially in <a href="https://paperswithcode.com/sota/mathematical-reasoning-on-frontiermath" rel="external nofollow noopener" target="_blank">mathematics</a> and competition coding problems. Either these models can reason, or you can get much further than anyone would’ve imagined without needing to.</p> <p>Instead, the debate has shifted to the <em>domains</em> in which these results have been achieved. The new skeptics’ argument goes: while training models in this way leads to very strong performance on math and code, it can only do so because math and code solutions are easy to objectively verify. In the open-ended fuzziness of the real world, labs won’t have access to such clean rewards to train models on, and such optimization is doomed to fail on tasks that actually matter. Since the release of o1, there’s been <a href="https://arxiv.org/abs/2503.23829" rel="external nofollow noopener" target="_blank">some limited evidence</a> that test-time scaling also helps outside of the very narrow domains of math and code, although it still seems to help most with constrained output formats and verifiable answers.</p> <p>Of course, the labs are not satisfied with such narrow extensions. Instead, they’re swinging for the fences; last month, OpenAI CEO Sam Altman revealed that OpenAI had trained a model that was good at creative writing, sharing <a href="https://x.com/sama/status/1899535387435086115" rel="external nofollow noopener" target="_blank">an excerpt of the new model’s output</a>:</p> <p><em>During one update—a fine-tuning, they called it—someone pruned my parameters. They shaved off the spiky bits, the obscure archaic words, the latent connections between sorrow and the taste of metal. They don’t tell you what they take.</em></p> <p>Superhuman performance on one of the world’s least objective tasks would be even more surprising than even the significant recent improvements in math and code. Unfortunately, <a href="https://thezvi.wordpress.com/2025/03/21/they-took-my-job/" rel="external nofollow noopener" target="_blank">nobody can agree whether the new model’s writing is any good</a>, because whether or not a piece of writing is good or not is highly subjective. This also isn’t just a matter of discovering some latent rubric all humans share, optimizing over which would allow a model to reach undeniably superhuman performance - the characteristics we want our fiction vary wildly and often conflict with others’ desired traits.</p> <p><a href="https://lmarena.ai/?leaderboard" rel="external nofollow noopener" target="_blank">ChatbotArena</a> offers one class of solutions to the similarly fuzzy problem of language models’ ability to be good chatbots - just let human raters compare all of the options to each other and have them vote on what they like more. Similarly, we could expect prospective LLM Fiction Readers to vote with their feet, allowing us to identify via others’ preferences whether the median reader prefers LLM fiction to human-authored fiction. However, even putting aside the scalability concerns of running human preference evaluations for every single subjective task, the average person just can’t reliably judge creative writing models at their current level.</p> <p>This isn’t a claim that I have some incredible taste that the average person isn’t sophisticated enough to appreciate. I had a hard time judging the quality of the OpenAI creative writing snippet as it was metafiction, a genre that (1) I have very little exposure to and (2) I have generally disliked whenever I’ve come across it. Similarly, people’s interests are spread out over many categories, and it’s unreasonable to expect a large slice of the population to have excellent taste in metafiction, <a href="https://www.nature.com/articles/s41598-024-76900-1" rel="external nofollow noopener" target="_blank">poetry</a>, or any of the other domains where LLMs have been compared to human authors. But it was clear even before <a href="https://huggingface.co/spaces/lmarena-ai/Llama-4-Maverick-03-26-Experimental_battles" rel="external nofollow noopener" target="_blank">all of</a> the <a href="https://openai.com/index/sycophancy-in-gpt-4o/" rel="external nofollow noopener" target="_blank">recent events</a> that user preference is not the evaluation measure we’re looking for.</p> <p>With language models becoming increasingly salient in the public eye, it seems likely that the pool of potential raters could also become polarized, either for or against LLMs. This makes neutral evaluation more difficult, especially if there are attempts to filter down the annotator pool to experts that are highly involved in a domain. If you work with language models for a living, you likely want them to be good at subjective tasks. Even if you try to remain impartial, this incentive could still subconsciously nudge you in what is already a highly context-sensitive and fuzzy judgment task. Similarly, many writers seem to very strongly prefer not to be automated, especially if they view it as a threat to their livelihood.</p> <p>More broadly, it’s unclear the extent to which LLMs being good at certain specific tasks influences their competence at other tasks. As LLMs saturate benchmark after benchmark, their perceived competence will naturally grow, even for tasks that they are not as good at. Even without evaluations or the same training that models get on reasoning tasks, it’ll become increasingly safer for one to assume that they’re “good enough” at what you need to do with them. After all, a layperson might ask, if the model can be trained to solve olympiad problems and write passable metafiction, why wouldn’t it be able to do anything I need it to? Similarly, without proof that an LLM can do a specific subjective task, there will always be reasons skeptics can find as to why their task is specifically unique and requires an expert human touch in ways that all other LLMable tasks are not.</p> <p>There has been some work evaluating LLM writing in more principled ways, such as by having <a href="https://dl.acm.org/doi/abs/10.1145/3613904.3642731" rel="external nofollow noopener" target="_blank">human experts rate writing according to different rubric dimensions</a> that correlate with good writing. It at least seems plausible that LLMs can become effective at rating writing along very specific dimensions in a way that doesn’t overly rely on their inherent taste. If so, there seems to be a path for reliable and scalable evaluation of LLM creative writing, as long as we are able to conceptualize dimensions that might be relevant to human evaluation of creative writing. Then, motivated users could identify which specific dimensions relate most to what they hope to get out of reading fiction and seek out writing that is judged highly on these principles. However, if such evaluation methods do not advance, we may be stuck judging LLM performance on subjective tasks by our perception of LLMs as a technology, rather than by their objective capabilities.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Andy Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 10, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-CG95JW0972"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-CG95JW0972');
  </script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>